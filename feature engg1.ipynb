{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb376b0f",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed4b28",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data for one or more variables in a particular observation. In other words, it is a situation where some values are not recorded or not available for some of the observations in the dataset.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can cause biased or incorrect analyses if left untreated. Missing data can affect the accuracy and validity of statistical analyses and machine learning models. For example, if a model relies on a variable with missing values, it may not be able to make accurate predictions or generate reliable insights.\n",
    "\n",
    "Some algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting. These algorithms can handle missing data by treating missing values as a separate category or by using surrogate variables to impute missing data. Other algorithms, such as logistic regression and neural networks, may require the data to be imputed or removed before the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8ee10",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f525f",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in a dataset. Here are some of the most commonly used techniques, along with an example of each in Python:\n",
    "\n",
    "Deletion: In this technique, the rows or columns with missing values are deleted from the dataset. This method is straightforward but can result in a loss of information if a large number of rows or columns are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02814e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [A, B, C]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, np.nan], \n",
    "                   'B': [5, np.nan, 7, np.nan, 9], \n",
    "                   'C': [np.nan, 11, 12, np.nan, 14]})\n",
    "\n",
    "# drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(df_dropped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cf3c8",
   "metadata": {},
   "source": [
    "Imputation: In this technique, the missing values are replaced with estimated values based on the available data. There are several methods for imputing missing data, including mean imputation, median imputation, and regression imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3617f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B          C\n",
      "0  1.000000  5.0  12.333333\n",
      "1  2.000000  7.0  11.000000\n",
      "2  2.333333  7.0  12.000000\n",
      "3  4.000000  7.0  12.333333\n",
      "4  2.333333  9.0  14.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, np.nan], \n",
    "                   'B': [5, np.nan, 7, np.nan, 9], \n",
    "                   'C': [np.nan, 11, 12, np.nan, 14]})\n",
    "\n",
    "# impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b40c0",
   "metadata": {},
   "source": [
    "Model-based imputation: In this technique, missing values are imputed based on a predictive model trained on the non-missing data. This method can be more accurate than simple imputation methods but can also be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b610be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B          C\n",
      "0  1.000000  5.000000  12.333269\n",
      "1  2.000000  6.999981  11.000000\n",
      "2  3.500013  7.000000  12.000000\n",
      "3  4.000000  6.999998  12.333317\n",
      "4  9.499971  9.000000  14.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, np.nan], \n",
    "                   'B': [5, np.nan, 7, np.nan, 9], \n",
    "                   'C': [np.nan, 11, 12, np.nan, 14]})\n",
    "\n",
    "# impute missing values using model-based imputation\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=0)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d3b06",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aabd01",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of the target variable in a dataset is skewed or uneven. In other words, one class or category of the target variable is much more prevalent than the other(s). For example, in a binary classification problem where the goal is to predict whether a customer will churn or not, if the dataset contains many more examples of customers who do not churn than those who do, the data is considered imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several issues, including:\n",
    "\n",
    "Bias in the model: In an imbalanced dataset, a model may be biased towards the majority class, resulting in poor performance on the minority class. This is because the model is optimized to minimize the overall error rate, which may not reflect the true performance on the minority class.\n",
    "\n",
    "Poor generalization: A model trained on an imbalanced dataset may not generalize well to new, unseen data. This is because the model may overfit to the majority class and fail to capture the underlying patterns in the minority class.\n",
    "\n",
    "Misleading performance metrics: In an imbalanced dataset, accuracy may not be a reliable metric for evaluating model performance. This is because a model that always predicts the majority class will have high accuracy but will be useless for predicting the minority class.\n",
    "\n",
    "To address imbalanced data, several techniques can be used, including oversampling the minority class, undersampling the majority class, using cost-sensitive learning, and using ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32e111",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1d71d",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to address imbalanced data by modifying the distribution of the target variable in a dataset.\n",
    "\n",
    "Up-sampling involves increasing the number of instances in the minority class to balance the distribution with the majority class. This can be achieved by duplicating instances from the minority class, or by generating new synthetic instances based on the existing data. The goal of up-sampling is to ensure that the model has enough examples of the minority class to learn from.\n",
    "\n",
    "Down-sampling, on the other hand, involves reducing the number of instances in the majority class to balance the distribution with the minority class. This can be achieved by randomly removing instances from the majority class. The goal of down-sampling is to reduce the bias towards the majority class and allow the model to learn from the minority class as well.\n",
    "\n",
    "An example of when up-sampling and down-sampling might be required is in a credit card fraud detection problem. Suppose we have a dataset of 10,000 credit card transactions, out of which only 100 are fraudulent. In this case, the data is heavily imbalanced, and if we train a model on this dataset, it is likely to be biased towards the non-fraudulent transactions. To address this, we can use up-sampling to generate synthetic instances of the minority class (i.e., the fraudulent transactions) and increase their representation in the dataset. Alternatively, we can use down-sampling to remove instances of the majority class (i.e., the non-fraudulent transactions) and balance the distribution with the minority class.\n",
    "\n",
    "In practice, the choice between up-sampling and down-sampling depends on several factors, including the size of the dataset, the complexity of the problem, and the available computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5705cc9",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59159d26",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new synthetic examples from the existing data. The goal of data augmentation is to improve the robustness and generalization of a machine learning model by exposing it to a wider variety of examples.\n",
    "\n",
    "One popular data augmentation technique for addressing imbalanced data is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE works by generating new synthetic examples of the minority class based on the existing examples. The algorithm works as follows:\n",
    "\n",
    "Choose a minority class example at random.\n",
    "Choose one of its k nearest neighbors (also from the minority class) at random.\n",
    "Generate a new synthetic example at a randomly chosen point between the two examples in the feature space.\n",
    "Repeat steps 1-3 until the desired balance between the classes is achieved.\n",
    "SMOTE can be a powerful technique for addressing imbalanced data, as it allows the model to learn from a wider variety of examples without requiring additional data collection. However, it is important to note that SMOTE and other data augmentation techniques should be used with caution, as they can lead to overfitting if not applied correctly. It is also important to carefully evaluate the performance of the model on a separate validation set to ensure that the synthetic examples are truly representative of the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6fd761",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb656b2",
   "metadata": {},
   "source": [
    "Outliers are data points that are significantly different from other data points in a dataset. Outliers can be caused by various factors, such as measurement errors, data entry errors, or extreme values that are legitimately present in the data.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the analysis of a dataset. Outliers can skew statistical analyses and lead to incorrect conclusions about the data. For example, if a dataset contains an outlier that is much larger than other values, the mean and standard deviation of the dataset will be affected, and any analyses that rely on these statistics will be biased. Outliers can also affect the performance of machine learning algorithms by causing overfitting or underfitting.\n",
    "\n",
    "Handling outliers involves identifying them and deciding how to treat them. This can be done using various techniques, including visualization, statistical tests, and machine learning models. Once the outliers are identified, they can be treated by either removing them from the dataset or adjusting their values to be more representative of the underlying data.\n",
    "\n",
    "Removing outliers can be an effective way to improve the analysis of a dataset, but it is important to be cautious when doing so, as it can also lead to loss of important information. Adjusting the values of outliers can be a more nuanced approach that preserves the overall structure of the data while reducing the impact of outliers on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98529d",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some ofthe data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745357d1",
   "metadata": {},
   "source": [
    "dropna(), fillna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56573908",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What aresome strategies you can use to determine if the missing data is missing at random or if there is a patternto the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637666aa",
   "metadata": {},
   "source": [
    "drop the small portion of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57061e0f",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in thedataset do not have the condition of interest, while a small percentage do. What are some strategies youcan use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a71628",
   "metadata": {},
   "source": [
    "When working with an imbalanced dataset, it is important to use evaluation metrics that take into account the class distribution and provide a more accurate picture of the model's performance. Some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced medical diagnosis dataset include:\n",
    "\n",
    "Confusion Matrix: A confusion matrix can be used to visualize the performance of the model and determine how many true positives, true negatives, false positives, and false negatives the model produced. This can help to identify which class the model is having difficulty with.\n",
    "\n",
    "ROC Curve: The Receiver Operating Characteristic (ROC) curve is a graphical representation of the trade-off between true positive rate (TPR) and false positive rate (FPR) at various classification thresholds. This curve provides a useful summary of the model's performance across all possible thresholds.\n",
    "\n",
    "Precision-Recall Curve: Precision-Recall curve is another graphical representation of the model's performance that is useful when dealing with imbalanced datasets. This curve shows the trade-off between precision and recall at various classification thresholds.\n",
    "\n",
    "F1 Score: The F1 score is a metric that combines precision and recall into a single score. It is a useful metric for imbalanced datasets because it takes into account both false positives and false negatives.\n",
    "\n",
    "AUC-ROC: The Area Under the Curve (AUC) of the ROC curve is a common metric used to evaluate the performance of a binary classifier on an imbalanced dataset. It provides a single number that summarizes the model's performance across all possible classification thresholds.\n",
    "\n",
    "It is also important to use appropriate sampling techniques, such as up-sampling, down-sampling, or data augmentation to balance the dataset before training the model. Cross-validation should also be used to ensure that the model's performance is consistent across different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903a792",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset isunbalanced, with the bulk of customers reporting being satisfied. What methods can you employ tobalance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447f384",
   "metadata": {},
   "source": [
    "downsampling technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c612d",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on aproject that requires you to estimate the occurrence of a rare event. What methods can you employ tobalance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81c303",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a42026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
